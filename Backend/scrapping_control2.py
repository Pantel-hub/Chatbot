# Imports Î±Ï€ÏŒ Ï„Î± Ï…Ï€Î¬ÏÏ‡Î¿Î½Ï„Î± modules
from source_code import get_website_source_code
from link_discovery import get_detailed_links_info
from clean_html import clean_html_for_content

# Standard library imports
import json
from typing import Dict, List, Optional, Any
import time
from bs4 import BeautifulSoup
import asyncio
from concurrent.futures import ThreadPoolExecutor
import logging

logger = logging.getLogger(__name__)


class ScrapingController:
    """
    Controller Î³Î¹Î± unified web scraping Ï€Î¿Ï… ÎµÎ½Î¿Ï€Î¿Î¹ÎµÎ¯ source_code, link_discovery ÎºÎ±Î¹ clean_html
    """

    def __init__(self, timeout: int = 8, headless: bool = True):
        self.timeout = timeout
        self.headless = headless
        # Ï€Î±ÏÎ¬Î»Î»Î·Î»ÎµÏ‚ ÎµÏÎ³Î±ÏƒÎ¯ÎµÏ‚
        self.executor = ThreadPoolExecutor(max_workers=5)

    def scrape_website(self, url: str) -> Dict[str, Any]:
        """
        Scrape Î¼Î¯Î± ÏƒÎµÎ»Î¯Î´Î± ÎºÎ±Î¹ ÏŒÎ»Î± Ï„Î± discovered links Ï„Î·Ï‚

        Returns:
            Dictionary Î¼Îµ Ï„Î¿ JSON format Ï€Î¿Ï… Î¿ÏÎ¯ÏƒÎ±Î¼Îµ
        """
        result = {
            "main_page": {},
            "discovered_links": [],
            "summary": {"total_links_found": 0, "successfully_scraped": 0, "failed": 0},
        }

        # 1. Scrape Ï„Î·Î½ ÎºÏÏÎ¹Î± ÏƒÎµÎ»Î¯Î´Î±
        print(f"Scraping main page: {url}")
        main_page_data = self._scrape_single_page(url)
        result["main_page"] = main_page_data

        # 2. Discover links Î±Ï€ÏŒ Ï„Î·Î½ ÎºÏÏÎ¹Î± ÏƒÎµÎ»Î¯Î´Î±
        if main_page_data.get("status") == "success":
            print("Discovering links...")

            raw_html = main_page_data.get("raw_html")
            links_info = get_detailed_links_info(
                html_content=raw_html, base_url=url, include_external=False
            )

            if links_info.get("success"):
                all_links = links_info.get("links", [])
                result["summary"]["total_links_found"] = len(all_links)

                links_to_scrape = all_links

                # 3. Scrape Ï„Î± discovered links
                for link_url in links_to_scrape:
                    print(f"Scraping link: {link_url}")
                    link_data = self._scrape_single_page(link_url)
                    result["discovered_links"].append(link_data)

                    if link_data.get("status") == "success":
                        result["summary"]["successfully_scraped"] += 1
                    else:
                        result["summary"]["failed"] += 1

        return result

    async def scrape_website_async(self, url: str) -> Dict[str, Any]:
        result = {
            "main_page": {},
            "discovered_links": [],
            "summary": {"total_links_found": 0, "successfully_scraped": 0, "failed": 0},
        }

        # 1. Scrape Ï„Î·Î½ ÎºÏÏÎ¹Î± ÏƒÎµÎ»Î¯Î´Î±
        print(f"Scraping main page: {url}")
        main_page_data = await self._scrape_single_page_async(url)
        result["main_page"] = main_page_data

        # 2. Discover links Î±Ï€ÏŒ Ï„Î·Î½ ÎºÏÏÎ¹Î± ÏƒÎµÎ»Î¯Î´Î±
        if main_page_data.get("status") == "success":
            print("Discovering links...")

            # âœ… Î‘Î›Î›Î‘Î“Î—: Î§ÏÎ·ÏƒÎ¹Î¼Î¿Ï€Î¿Î¯Î·ÏƒÎµ Ï„Î¿ HTML Ï€Î¿Ï… Î®Î´Î· Î­Ï‡ÎµÎ¹Ï‚
            raw_html = main_page_data.get("raw_html")
            links_info = get_detailed_links_info(
                html_content=raw_html, base_url=url, include_external=False
            )

            if links_info.get("success"):
                all_links = links_info.get("links", [])
                result["summary"]["total_links_found"] = len(all_links)

                links_to_scrape = all_links

                # 3. PARALLEL scraping Ï„Ï‰Î½ discovered links
                print(f"Starting parallel scraping of {len(links_to_scrape)} links...")
                tasks = [
                    self._scrape_single_page_async(link_url)
                    for link_url in links_to_scrape
                ]

                # Î•ÎºÏ„Î­Î»ÎµÏƒÎ· ÏŒÎ»Ï‰Î½ Ï„Ï‰Î½ tasks Ï€Î±ÏÎ¬Î»Î»Î·Î»Î±
                link_results = await asyncio.gather(*tasks, return_exceptions=True)

                # Î•Ï€ÎµÎ¾ÎµÏÎ³Î±ÏƒÎ¯Î± Î±Ï€Î¿Ï„ÎµÎ»ÎµÏƒÎ¼Î¬Ï„Ï‰Î½
                for link_data in link_results:
                    if isinstance(link_data, Exception):
                        # Handle exception
                        result["summary"]["failed"] += 1
                        continue

                    result["discovered_links"].append(link_data)

                    if link_data.get("status") == "success":
                        result["summary"]["successfully_scraped"] += 1
                    else:
                        result["summary"]["failed"] += 1

        return result

    def _scrape_single_page(self, url: str) -> Dict[str, Any]:
        """
        Scrape Î¼Î¯Î± Î¼Î¿Î½Î® ÏƒÎµÎ»Î¯Î´Î± ÎºÎ±Î¹ ÎµÏ€Î¹ÏƒÏ„ÏÎ­Ï†ÎµÎ¹ Ï„Î± Î´ÎµÎ´Î¿Î¼Î­Î½Î± Ï„Î·Ï‚

        Args:
            url: Î¤Î¿ URL Ï„Î·Ï‚ ÏƒÎµÎ»Î¯Î´Î±Ï‚

        Returns:
            Dictionary Î¼Îµ Ï„Î± Î´ÎµÎ´Î¿Î¼Î­Î½Î± Ï„Î·Ï‚ ÏƒÎµÎ»Î¯Î´Î±Ï‚
        """
        page_data = {
            "url": url,
            "title": "",
            "clean_content": "",
            "status": "failed",
            "error": None,
        }

        # 1. Î Î¬ÏÎµ raw HTML Î±Ï€ÏŒ Bright Data
        try:
            raw_html = get_website_source_code(url, headless=self.headless)

            # âœ… Î Î¡ÎŸÎ£Î˜Î—ÎšÎ—: ÎšÏÎ¬Ï„Î± Ï„Î¿ raw HTML Î³Î¹Î± link discovery
            page_data["raw_html"] = raw_html

            if not raw_html:
                page_data["error"] = "Failed to fetch HTML"
                return page_data

            # 2. ÎšÎ±Î¸Î¬ÏÎ¹ÏƒÎµ Ï„Î¿ HTML
            clean_content = clean_html_for_content(raw_html)

            if clean_content:
                page_data["clean_content"] = clean_content.strip()
                page_data["status"] = "success"

                # Î•Î¾Î±Î³Ï‰Î³Î® title Î±Ï€ÏŒ Ï„Î¿ raw HTML
                try:
                    soup = BeautifulSoup(raw_html, "html.parser")
                    title_tag = soup.find("title")
                    if title_tag and title_tag.get_text():
                        page_data["title"] = title_tag.get_text().strip()
                except:
                    pass  # Î‘Î½ Î´ÎµÎ½ Î¼Ï€Î¿ÏÎ¿ÏÎ¼Îµ Î½Î± Ï€Î¬ÏÎ¿Ï…Î¼Îµ title, ÏƒÏ…Î½ÎµÏ‡Î¯Î¶Î¿Ï…Î¼Îµ

                # ÎœÎµÏ„Î±Ï„ÏÎ¿Ï€Î® Ï„Î¿Ï… clean_content ÏƒÎµ Î±Ï€Î»ÏŒ ÎºÎµÎ¯Î¼ÎµÎ½Î¿ (plain text)
                try:
                    soup_clean = BeautifulSoup(clean_content, "html.parser")
                    plain_text = soup_clean.get_text(separator="\n")
                    lines = [line.strip() for line in plain_text.splitlines()]
                    lines = [ln for ln in lines if ln]  # Ï€Î­Ï„Î± ÎºÎµÎ½Î­Ï‚ Î³ÏÎ±Î¼Î¼Î­Ï‚
                    plain_text = "\n".join(lines)
                    page_data["plain_text"] = plain_text
                    page_data["text_length"] = len(plain_text)
                    page_data["text_excerpt"] = plain_text[:500]
                except Exception:
                    page_data["plain_text"] = ""
                    page_data["text_length"] = 0
                    page_data["text_excerpt"] = ""

            else:
                page_data["error"] = "Failed to clean HTML"

        except Exception as e:
            import traceback
            page_data["error"] = str(e)
            logger.error(f"âŒ Error scraping {url}: {str(e)}")
            logger.error(f"ðŸ“ Full traceback:\n{traceback.format_exc()}")

        return page_data

    # Ï„Î¿ scraping ÎµÎºÏ„ÎµÎ»ÎµÎ¯Ï„Î±Î¹ ÏƒÎµ Î¾ÎµÏ‡Ï‰ÏÎ¹ÏƒÏ„ÏŒ thread
    async def _scrape_single_page_async(self, url: str) -> Dict[str, Any]:
        loop = asyncio.get_event_loop()
        return await loop.run_in_executor(self.executor, self._scrape_single_page, url)
